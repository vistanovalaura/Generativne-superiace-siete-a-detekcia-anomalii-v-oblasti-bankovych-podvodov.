{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Generatívne súperiace siete a detekcia anomálií v oblasti bankových podvodov**\n",
        "\n",
        "*Diplomová práca*\n",
        "\n",
        "Bc. Laura Vištanová\n",
        "\n",
        "Školiteľ: RNDr. Ľubomír Antoni, PhD. "
      ],
      "metadata": {
        "id": "-vx_ldfL0aKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dáta k diplomovej práci sme prevzali zo súťaže na stránke kaggle.com. Súťaž nájdeme na nasledujúcej stránke: https://www.kaggle.com/competitions/ieee-fraud-detection/overview\n",
        "\n",
        "Ako základ sme si zobrali víťazov tejto súťaže (odkaz nižšie), predspracovanie dát sme prevzali od nich, všetky ostatné časti sú už ale výsledkom našej práce. "
      ],
      "metadata": {
        "id": "PAd7ljsxwlOS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4c2uK8jFruQ"
      },
      "source": [
        "#**Import knižníc**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqCF34PPFyEu"
      },
      "outputs": [],
      "source": [
        "import numpy as np, pandas as pd, os, gc\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        " \n",
        "import glob\n",
        "from os import listdir\n",
        "from os.path import isdir\n",
        "from random import choice\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import model_selection\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8BbIXAQYr6G"
      },
      "source": [
        "# ***Úprava dátovej sady***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Túto časť práce sme urobili podľa víťazného tímu súťaže. Pôvodné úpravy nájdeme na stránke: https://www.kaggle.com/code/cdeotte/xgb-fraud-with-magic-0-9600 "
      ],
      "metadata": {
        "id": "IJQcyZ30wXk-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhYV2n-LYzt9"
      },
      "outputs": [],
      "source": [
        "BUILD95 = True\n",
        "BUILD96 = True\n",
        "\n",
        "import numpy as np, pandas as pd, os, gc\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# stĺpce so stringami \n",
        "str_type = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain','M1', 'M2', 'M3', 'M4','M5',\n",
        "            'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_30', \n",
        "            'id_31', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\n",
        "str_type += ['id-12', 'id-15', 'id-16', 'id-23', 'id-27', 'id-28', 'id-29', 'id-30', \n",
        "            'id-31', 'id-33', 'id-34', 'id-35', 'id-36', 'id-37', 'id-38']\n",
        "\n",
        "# prvých 53 stĺpcov\n",
        "cols = ['TransactionID', 'TransactionDT', 'TransactionAmt',\n",
        "       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n",
        "       'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain',\n",
        "       'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11',\n",
        "       'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8',\n",
        "       'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4',\n",
        "       'M5', 'M6', 'M7', 'M8', 'M9']\n",
        "\n",
        "# stĺpce v rozdelené podľa korelácie \n",
        "# https://www.kaggle.com/cdeotte/eda-for-columns-v-and-id\n",
        "v =  [1, 3, 4, 6, 8, 11]\n",
        "v += [13, 14, 17, 20, 23, 26, 27, 30]\n",
        "v += [36, 37, 40, 41, 44, 47, 48]\n",
        "v += [54, 56, 59, 62, 65, 67, 68, 70]\n",
        "v += [76, 78, 80, 82, 86, 88, 89, 91]\n",
        "\n",
        "v += [107, 108, 111, 115, 117, 120, 121, 123] # možno skupina, neobsahuje NaN\n",
        "v += [124, 127, 129, 130, 136] # koreluje so skupinami, neobsahuje NaN\n",
        "\n",
        "# obsahujú veľa NaN \n",
        "v += [138, 139, 142, 147, 156, 162] #b1\n",
        "v += [165, 160, 166] #b1\n",
        "v += [178, 176, 173, 182] #b2\n",
        "v += [187, 203, 205, 207, 215] #b2\n",
        "v += [169, 171, 175, 180, 185, 188, 198, 210, 209] #b2\n",
        "v += [218, 223, 224, 226, 228, 229, 235] #b3\n",
        "v += [240, 258, 257, 253, 252, 260, 261] #b3\n",
        "v += [264, 266, 267, 274, 277] #b3\n",
        "v += [220, 221, 234, 238, 250, 271] #b3\n",
        "\n",
        "v += [294, 284, 285, 286, 291, 297] # koreluje so skupinami, neobsahuje NaN\n",
        "v += [303, 305, 307, 309, 310, 320] # koreluje so skupinami, neobsahuje NaN\n",
        "v += [281, 283, 289, 296, 301, 314] # koreluje so skupinami, neobsahuje NaN\n",
        "\n",
        "cols += ['V'+str(x) for x in v]\n",
        "dtypes = {}\n",
        "for c in cols+['id_0'+str(x) for x in range(1,10)]+['id_'+str(x) for x in range(10,34)]+\\\n",
        "    ['id-0'+str(x) for x in range(1,10)]+['id-'+str(x) for x in range(10,34)]:\n",
        "        dtypes[c] = 'float32'\n",
        "for c in str_type: dtypes[c] = 'category'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZBCfchvY8zu"
      },
      "outputs": [],
      "source": [
        "#keby boli dáta na Google Drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg-v6xulY_zz"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# načítanie tréningových dát \n",
        "X_train = pd.read_csv('train_transaction.csv',index_col='TransactionID', dtype=dtypes, usecols=cols+['isFraud'])\n",
        "train_id = pd.read_csv('train_identity.csv',index_col='TransactionID', dtype=dtypes)\n",
        "X_train = X_train.merge(train_id, how='left', left_index=True, right_index=True)\n",
        "# načítanie testovacích dát \n",
        "X_test = pd.read_csv('test_transaction.csv',index_col='TransactionID', dtype=dtypes, usecols=cols)\n",
        "test_id = pd.read_csv('test_identity.csv',index_col='TransactionID', dtype=dtypes)\n",
        "fix = {o:n for o, n in zip(test_id.columns, train_id.columns)}\n",
        "test_id.rename(columns=fix, inplace=True)\n",
        "X_test = X_test.merge(test_id, how='left', left_index=True, right_index=True)\n",
        "# target\n",
        "y_train = X_train['isFraud'].copy()\n",
        "del train_id, test_id, X_train['isFraud']; x = gc.collect()\n",
        "\n",
        "print('Train shape',X_train.shape,'test shape',X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0n-LwbXZCKU"
      },
      "outputs": [],
      "source": [
        "# zobrazenie pôvodných D stĺpcov\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.scatter(X_train.TransactionDT,X_train.D15)\n",
        "plt.title('Original D15')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('D15')\n",
        "plt.show()\n",
        "\n",
        "# normalizácia D stĺpcov\n",
        "for i in range(1,16):\n",
        "    if i in [1,2,3,5,9]: continue\n",
        "    X_train['D'+str(i)] =  X_train['D'+str(i)] - X_train.TransactionDT/np.float32(24*60*60)\n",
        "    X_test['D'+str(i)] = X_test['D'+str(i)] - X_test.TransactionDT/np.float32(24*60*60) \n",
        "\n",
        "# zobrazenie transformovaných D stĺpcov \n",
        "plt.figure(figsize=(15,5))\n",
        "plt.scatter(X_train.TransactionDT,X_train.D15)\n",
        "plt.title('Transformed D15')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('D15n')\n",
        "plt.show()\n",
        "\n",
        "%%time\n",
        "# zníženie nárokov na pamäť\n",
        "for i,f in enumerate(X_train.columns):\n",
        "    # faktorizácia kategoriálnych premenných\n",
        "    if (np.str(X_train[f].dtype)=='category')|(X_train[f].dtype=='object'): \n",
        "        df_comb = pd.concat([X_train[f],X_test[f]],axis=0)\n",
        "        df_comb,_ = df_comb.factorize(sort=True)\n",
        "        if df_comb.max()>32000: print(f,'needs int32')\n",
        "        X_train[f] = df_comb[:len(X_train)].astype('int16')\n",
        "        X_test[f] = df_comb[len(X_train):].astype('int16')\n",
        "    # numerické premenné na pozitívne, NaN hodnoty na -1\n",
        "    elif f not in ['TransactionAmt','TransactionDT']:\n",
        "        mn = np.min((X_train[f].min(),X_test[f].min()))\n",
        "        X_train[f] -= np.float32(mn)\n",
        "        X_test[f] -= np.float32(mn)\n",
        "        X_train[f].fillna(-1,inplace=True)\n",
        "        X_test[f].fillna(-1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtI-lVteZETY"
      },
      "outputs": [],
      "source": [
        "# frequency encode\n",
        "def encode_FE(df1, df2, cols):\n",
        "    for col in cols:\n",
        "        df = pd.concat([df1[col],df2[col]])\n",
        "        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n",
        "        vc[-1] = -1\n",
        "        nm = col+'_FE'\n",
        "        df1[nm] = df1[col].map(vc)\n",
        "        df1[nm] = df1[nm].astype('float32')\n",
        "        df2[nm] = df2[col].map(vc)\n",
        "        df2[nm] = df2[nm].astype('float32')\n",
        "        print(nm,', ',end='')\n",
        "        \n",
        "# label encode\n",
        "def encode_LE(col,train=X_train,test=X_test,verbose=True):\n",
        "    df_comb = pd.concat([train[col],test[col]],axis=0)\n",
        "    df_comb,_ = df_comb.factorize(sort=True)\n",
        "    nm = col\n",
        "    if df_comb.max()>32000: \n",
        "        train[nm] = df_comb[:len(train)].astype('int32')\n",
        "        test[nm] = df_comb[len(train):].astype('int32')\n",
        "    else:\n",
        "        train[nm] = df_comb[:len(train)].astype('int16')\n",
        "        test[nm] = df_comb[len(train):].astype('int16')\n",
        "    del df_comb; x=gc.collect()\n",
        "    if verbose: print(nm,', ',end='')\n",
        "        \n",
        "# agregácia skupín\n",
        "# https://www.kaggle.com/kyakovlev/ieee-fe-with-some-eda\n",
        "def encode_AG(main_columns, uids, aggregations=['mean'], train_df=X_train, test_df=X_test, \n",
        "              fillna=True, usena=False):\n",
        "    # agregácia pomocou UID pre danú štatistiku \n",
        "    for main_column in main_columns:  \n",
        "        for col in uids:\n",
        "            for agg_type in aggregations:\n",
        "                new_col_name = main_column+'_'+col+'_'+agg_type\n",
        "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
        "                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n",
        "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
        "                                                        columns={agg_type: new_col_name})\n",
        "\n",
        "                temp_df.index = list(temp_df[col])\n",
        "                temp_df = temp_df[new_col_name].to_dict()   \n",
        "\n",
        "                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n",
        "                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n",
        "                \n",
        "                if fillna:\n",
        "                    train_df[new_col_name].fillna(-1,inplace=True)\n",
        "                    test_df[new_col_name].fillna(-1,inplace=True)\n",
        "                \n",
        "                print(\"'\"+new_col_name+\"'\",', ',end='')\n",
        "                \n",
        "# kombinácia atribútov\n",
        "def encode_CB(col1,col2,df1=X_train,df2=X_test):\n",
        "    nm = col1+'_'+col2\n",
        "    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n",
        "    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n",
        "    encode_LE(nm,verbose=False)\n",
        "    print(nm,', ',end='')\n",
        "    \n",
        "# agregácia neunikátnych skupín\n",
        "def encode_AG2(main_columns, uids, train_df=X_train, test_df=X_test):\n",
        "    for main_column in main_columns:  \n",
        "        for col in uids:\n",
        "            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n",
        "            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n",
        "            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n",
        "            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n",
        "            print(col+'_'+main_column+'_ct, ',end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xtECeIjZSME"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# TransactionAmt v centoch \n",
        "X_train['cents'] = (X_train['TransactionAmt'] - np.floor(X_train['TransactionAmt'])).astype('float32')\n",
        "X_test['cents'] = (X_test['TransactionAmt'] - np.floor(X_test['TransactionAmt'])).astype('float32')\n",
        "print('cents, ', end='')\n",
        "# frequency encode: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN\n",
        "encode_FE(X_train,X_test,['addr1','card1','card2','card3','P_emaildomain'])\n",
        "# kombinácia atribútov CARD1+ADDR1, CARD1+ADDR1+P_EMAILDOMAIN\n",
        "encode_CB('card1','addr1')\n",
        "encode_CB('card1_addr1','P_emaildomain')\n",
        "# frequency encode\n",
        "encode_FE(X_train,X_test,['card1_addr1','card1_addr1_P_emaildomain'])\n",
        "# agregácia skupín\n",
        "encode_AG(['TransactionAmt','D9','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],usena=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYyjN5O8ZW_S"
      },
      "outputs": [],
      "source": [
        "cols = list( X_train.columns )\n",
        "cols.remove('TransactionDT')\n",
        "for c in ['D6','D7','D8','D9','D12','D13','D14']:\n",
        "    cols.remove(c)\n",
        "    \n",
        "for c in ['C3','M5','id_08','id_33']:\n",
        "    cols.remove(c)\n",
        "for c in ['card4','id_07','id_14','id_21','id_30','id_32','id_34']:\n",
        "    cols.remove(c)\n",
        "for c in ['id_'+str(x) for x in range(22,28)]:\n",
        "    cols.remove(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJYyXMu4ZZmQ"
      },
      "outputs": [],
      "source": [
        "#uloženie dát \n",
        "\n",
        "X_train.to_csv('train_transaction_po_uprave.csv', index=True)\n",
        "y_train.to_csv('y_train.csv', index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pIAtjNxkZ85"
      },
      "source": [
        "#***Výber atribútov***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUTASszc7Mmj"
      },
      "outputs": [],
      "source": [
        "#kompletne cele data \n",
        "X_train = pd.read_csv('train_transaction_po_uprave.csv')\n",
        "y_train = pd.read_csv('y_train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AB1cPeAZCizc"
      },
      "outputs": [],
      "source": [
        "#normálizácia dát pre lepšie fungovanie algoritmov\n",
        "from sklearn import preprocessing\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "names = X_train.columns\n",
        "d = scaler.fit_transform(X_train)\n",
        "scaled_df_train = pd.DataFrame(d, columns=names)\n",
        "scaled_df_train.tail()\n",
        "\n",
        "X_train = scaled_df_train\n",
        "del X_train[X_train.columns[0]]\n",
        "\n",
        "#pre istotu, keby náhodou boli NaN hodnoty v tabuľke \n",
        "X_train = X_train.dropna(axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CoMKX1C7SmU"
      },
      "outputs": [],
      "source": [
        "#SelectKBest \n",
        "#score_func - funkcia pomocou ktorej vyberáme atribúty, používali sme chi2 a f_classif\n",
        "#k - počet atribútov, ktoré chceme vybrať. My sme použili počet 30, ale môže sa zmeniť \n",
        "\n",
        "#importy k výberu atribútov\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "X = X_train\n",
        "#aby sme mali stĺpec isFraud \n",
        "y_train = pd.DataFrame(y_train)\n",
        "y = y_train['isFraud']    \n",
        "\n",
        "#uvádzame príkazy s oboma funkciami\n",
        "bestfeatures = SelectKBest(score_func=chi2, k=30)\n",
        "#bestfeatures = SelectKBest(score_func=f_classif, k=25)\n",
        "fit = bestfeatures.fit(X,y)\n",
        "dfscores = pd.DataFrame(fit.scores_)\n",
        "dfcolumns = pd.DataFrame(X.columns)\n",
        "#spojíme predchádzajúce dataframy aby sa ľahšie čítali \n",
        "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
        "featureScores.columns = ['Specs','Score']  \n",
        "#vypíšeme prvých k atribútov\n",
        "print(featureScores.nlargest(30,'Score'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzRs4L73771n"
      },
      "outputs": [],
      "source": [
        "#ExtraTreesClassifier\n",
        "#max_depth - maximálna hĺbka stromov. My sme použili hodnotu 4, aby sa nevytvárali príliš veľké stromy, ale môže sa použiť aj vyššia hodnota. \n",
        "\n",
        "#importy k výberu atribútov\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = X_train\n",
        "#aby sme mali stĺpec isFraud \n",
        "y_train = pd.DataFrame(y_train)\n",
        "y = y_train['isFraud']    \n",
        "\n",
        "model = ExtraTreesClassifier(max_depth=4)\n",
        "model.fit(X,y)\n",
        "print(model.feature_importances_) \n",
        "#vizualizácia najlepších atribútov \n",
        "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "feat_importances.nlargest(30).plot(kind='barh')\n",
        "plt.show()\n",
        "#výpis najlepších atribútov\n",
        "feat_importances.nlargest(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V1W28f9Nm8D"
      },
      "outputs": [],
      "source": [
        "#opäť načítame celý dataset, aby sme z neho vytvorili nový dataset obsahujúci iba vybrané atribúty \n",
        "X_train = pd.read_csv('train_transaction_po_uprave.csv',index_col='TransactionID')\n",
        "y_train = pd.read_csv('y_train.csv')\n",
        "X_train = pd.merge(X_train, y_train, on = 'TransactionID')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpvlfW0xzsLF"
      },
      "outputs": [],
      "source": [
        "#vytvorenie nových datasetov. Pred každým spustením nasledujúcich troch buniek treba \n",
        "#spustiť aj predchádzajúcu, aby sme vychádzali vždy z pôvodného datasetu.\n",
        "\n",
        "\n",
        "#TOP 30 atribútov SelectKBest: chi\n",
        "X_train = X_train[['id_10', 'id_04', 'id_17', 'id_11', 'id_12', 'id_37', 'D9', 'DeviceType', 'id_06', 'id_09',\n",
        "                   'id_20', 'id_03', 'id_13', 'id_01', 'id_05', 'D12', 'id_38', 'id_36', 'id_28', 'id_29',\n",
        "                   'M4', 'D14', 'D7', 'DeviceInfo', 'id_35', 'id_16', 'id_15', 'id_31', 'D6', 'D13', 'isFraud']]\n",
        "X_train.to_csv('top30_SelectKBest_chi.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TOP 30 atribútov SelectKBest: f_classif\n",
        "X_train = X_train[['V257', 'V258', 'V228', 'V171', 'V188', 'V176', 'id_17', 'addr2', 'addr1_FE', 'ProductCD',\n",
        "                   'V198', 'V238', 'V185', 'V221', 'card3', 'id_09', 'V252', 'id_10', 'card3_FE', 'DeviceType',\n",
        "                   'V261', 'V229', 'id_03', 'V260', 'id_04', 'id_20', 'D12', 'id_37', 'id_12', 'D7' ,'isFraud']]\n",
        "X_train.to_csv('top30_SelectKBest_f.csv', index=False)"
      ],
      "metadata": {
        "id": "25sueCTL6yz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TOP 30 atribútov ExtraTreesClassifier: \n",
        "X_train = X_train[['V188', 'V257', 'id_17', 'V258', 'addr1_FE', 'id_35', 'id_09', 'addr2', 'id_10', 'V228', \n",
        "                   'D9', 'card3', 'V23', 'id_04', 'V86', 'V156', 'V47', 'V187', 'V171', 'id_12', \n",
        "                   'M4', 'D7', 'V54', 'D11_card1_std', 'V124', 'V44', 'id_01', 'V65', 'id_38' ,'isFraud']]\n",
        "X_train.to_csv('top30_ExtraTreesClassifier.csv', index=False)"
      ],
      "metadata": {
        "id": "lAdsWSdS6yoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MY43uNO4Nqs"
      },
      "source": [
        "#***tabGAN***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqOEseDpTGPE",
        "outputId": "01d4ec76-8c03-46a6-a4e3-f141e1bb870c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tabgan in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tabgan) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tabgan) (4.64.0)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.7/dist-packages (from tabgan) (2.2.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from tabgan) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from tabgan) (2.8.2)\n",
            "Requirement already satisfied: category-encoders in /usr/local/lib/python3.7/dist-packages (from tabgan) (2.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from tabgan) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from tabgan) (1.10.0+cu111)\n",
            "Requirement already satisfied: scikit-learn==0.23.2 in /usr/local/lib/python3.7/dist-packages (from tabgan) (0.23.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->tabgan) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->tabgan) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->tabgan) (1.4.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from category-encoders->tabgan) (0.10.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders->tabgan) (0.5.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tabgan) (2022.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.1->category-encoders->tabgan) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->tabgan) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->tabgan) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tabgan\n",
        "#keby nefungovala najnovšia verzia:\n",
        "#!pip install tabgan==1.1.1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhfmKEJV4ZGx"
      },
      "outputs": [],
      "source": [
        "X_train = pd.read_csv('top30_ExtraTreesClassifier.csv')\n",
        "\n",
        "fraud_transaction = X_train\n",
        "# 75 % dát dáme do tréningovej množiny, zvyšok do testovacej \n",
        "X_train = fraud_transaction.sample(frac=0.75, random_state=25)\n",
        "X_test = fraud_transaction.drop(X_train.index)\n",
        "y_train = X_train['isFraud']\n",
        "X_train = X_train.drop(columns=['isFraud'])\n",
        "X_test = X_test.drop(columns=['isFraud'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "L-XwF7fK4mo9",
        "outputId": "67fb4597-ee95-4da6-e8b2-489876ba01da"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ContextualVersionConflict",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mContextualVersionConflict\u001b[0m                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ebbc156d8936>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtabgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOriginalGenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGANGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# random input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tabgan/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Change here if project is renamed and does not equal the package name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdist_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"unknown\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected string, Requirement, or Distribution\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;34m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0mincluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meven\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mwere\u001b[0m \u001b[0malready\u001b[0m \u001b[0mactivated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mworking\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \"\"\"\n\u001b[0;32m--> 886\u001b[0;31m         \u001b[0mneeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneeded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    775\u001b[0m                 \u001b[0;31m# Oops, the \"best\" so far conflicts with a dependency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m                 \u001b[0mdependent_req\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequired_by\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mVersionConflict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependent_req\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# push the new requirements onto the stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mContextualVersionConflict\u001b[0m: (scikit-learn 1.0.2 (/usr/local/lib/python3.7/dist-packages), Requirement.parse('scikit-learn==0.23.2'), {'tabgan'})"
          ]
        }
      ],
      "source": [
        "#!pip install scikit_learn==0.23.2\n",
        "#je možné, že bude potrebný aj tento install, závisí od prostredia v ktorom spúšťame kódy. Ohľadom tejto veci sme komunikovali aj s autorom tohto algoritmu.\n",
        "\n",
        "from tabgan.sampler import OriginalGenerator, GANGenerator\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# random input data\n",
        "\n",
        "X_train1 = pd.DataFrame(X_train)\n",
        "X_test1 = pd.DataFrame(X_test)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "\n",
        "train = X_train1\n",
        "target = y_train\n",
        "test = X_test1\n",
        " \n",
        "#príklad generátora so základnými nastaveniami \n",
        "#new_train, new_target = GANGenerator().generate_data_pipe(train, target, test, )\n",
        "\n",
        "# príklad generátora s rozšírenými nastaveniami\n",
        "new_train, new_target = GANGenerator(gen_x_times=1.1, cat_cols=None,\n",
        "           bot_filter_quantile=0.001, top_filter_quantile=0.999, is_post_process=True,\n",
        "           adversaial_model_params={\n",
        "               \"metrics\": \"AUC\", \"max_depth\": 2, \"max_bin\": 100, \n",
        "               \"learning_rate\": 0.02, \"random_state\": 42, \"n_estimators\": 500,\n",
        "           }, pregeneration_frac=2, only_generated_data=False,\n",
        "           gan_params = {\"batch_size\": 500, \"patience\": 25, \"epochs\" : 500,}).generate_data_pipe(train, target,\n",
        "                                          test, deep_copy=True, only_adversarial=False, use_adversarial=True)\n",
        "                                       \n",
        "\n",
        "new_dataframe = new_train.join(new_target)\n",
        "new_dataframe.to_csv('model_tabGAN_ETC.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd3KjD2zGHSe"
      },
      "source": [
        "# **CTGAN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AfR6XDYp4iD"
      },
      "outputs": [],
      "source": [
        "from sdv.tabular import CTGAN\n",
        "import pandas as pd\n",
        "from ctgan import CTGANSynthesizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qBHSUuC_7TT0"
      },
      "outputs": [],
      "source": [
        "#načítanie datasetu \n",
        "X_train = pd.read_csv('top30_ExtraTreesClassifier.csv')\n",
        "\n",
        "#CTGAN so základnými nastaveniami \n",
        "ctgan = CTGAN()\n",
        "ctgan.fit(X_train)\n",
        "#uloženie modelu \n",
        "ctgan.save('model_CTGAN_ETC.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vytvorenie novej (rozšírenej) tréningovej množiny**"
      ],
      "metadata": {
        "id": "c0YKi-YxC0Zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#v prípade, že chceme použiť výstup z tabGAN\n",
        "\n",
        "#načítanie pôvodného datasetu \n",
        "X_train = pd.read_csv('top30_ExtraTreesClassifier.csv')\n",
        "y_train = X_train['isFraud']\n",
        "X_train = X_train.drop(columns=['isFraud'])\n",
        "\n",
        "#načítame umelú dátovú sadu, treba si dať pozor, aby sme načítali tú, ktorá patrí k datasetu X_train\n",
        "samples = pd.read_csv('model_tabGAN_ETC.csv')\n",
        "#vyberieme umelých podvodníkov\n",
        "podvodnici = samples.loc[samples['isFraud'] == 1]\n",
        "y_train_podvodnici = podvodnici['isFraud'].copy()\n",
        "podvodnici = podvodnici.drop(columns=['isFraud'])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "#rozdelíme pôvodné dáta na train/test \n",
        "X = X_train\n",
        "y = y_train\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "#vytvoríme novy train dataset s umelými podvodníkmi, test neobsahuje podvodné transakcie, ten dataset necháme v pôvodnom stave\n",
        "X_train = pd.concat([X_train, podvodnici])\n",
        "y_train = pd.concat([y_train, y_train_podvodnici])\n",
        "\n",
        "#pomiešame dáta\n",
        "X_train = X_train.join(y_train).sample(frac = 1)\n",
        "y_train = X_train['isFraud'].copy()\n",
        "X_train = X_train.drop(columns=['isFraud'])\n"
      ],
      "metadata": {
        "id": "IPPZkFxfC-DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#v prípade, že chceme použiť výstup z CTGAN\n",
        "\n",
        "#načítanie pôvodného datasetu \n",
        "X_train = pd.read_csv('top30_ExtraTreesClassifier.csv')\n",
        "y_train = X_train['isFraud']\n",
        "X_train = X_train.drop(columns=['isFraud'])\n",
        "\n",
        "#načítame model CTGAN, treba si dať pozor, aby sme načítali ten, ktorý patrí k datasetu X_train\n",
        "ctgan = CTGAN.load('model_CTGAN_ETC.pkl')\n",
        "\n",
        "#generujeme umelé dáta\n",
        "samples = ctgan.sample(10000)\n",
        "#vyberieme umelých podvodníkov\n",
        "podvodnici = samples.loc[samples['isFraud'] == 1]\n",
        "y_train_podvodnici = podvodnici['isFraud'].copy()\n",
        "podvodnici = podvodnici.drop(columns=['isFraud'])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "#rozdelíme pôvodné dáta na train/test \n",
        "X = X_train\n",
        "y = y_train\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "#vytvoríme novy train dataset s umelými podvodníkmi, test neobsahuje podvodné transakcie, ten dataset necháme v pôvodnom stave\n",
        "X_train = pd.concat([X_train, podvodnici])\n",
        "y_train = pd.concat([y_train, y_train_podvodnici])\n",
        "\n",
        "#pomiešame dáta\n",
        "X_train = X_train.join(y_train).sample(frac = 1)\n",
        "y_train = X_train['isFraud'].copy()\n",
        "X_train = X_train.drop(columns=['isFraud'])\n"
      ],
      "metadata": {
        "id": "EPpw0erFDPlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oliiQue-R66S"
      },
      "source": [
        "# **Rozhodovacie stromy**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#načítanie datasetu - bez umelých podvodníkov \n",
        "X = pd.read_csv('top30_ExtraTreesClassifier.csv')\n",
        "#cieľová premenná \n",
        "y = X['isFraud']\n",
        "X = X.drop(columns=['isFraud'])\n",
        "\n",
        "#vytvorenie tréningovej a testovacej množiny, do tréningovej množiny sme vložili 67 % dát, zvyšok do testovacej, tento parameter tiež môžeme zmeniť. \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "xGxvD3hC_6iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#použitie datasetu - s umelými podvodníkmi z časti Vytvorenie novej (rozšírenej) tréningovej množiny. \n",
        "#Tieto datasety by sa dali aj uložiť, kvôli šetreniu miesta sme ich neukladali stále zvlášť \n",
        "\n",
        "#pre istotu, keby náhodou boli NaN hodnoty v tabuľke, náhodný les s nimi nevie pracovať \n",
        "X_train = X_train.dropna(axis=1)\n",
        "\n",
        "y = y_train\n",
        "X = X_train\n",
        "\n",
        "#vytvorenie tréningovej a testovacej množiny, do tréningovej množiny sme vložili 67 % dát, zvyšok do testovacej, tento parameter tiež môžeme zmeniť. \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "dmHJsm9Vm2iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Náhodný les \n",
        "#použili sme 1000 stromov, ale tento parameter sa môže podľa potreby meniť, nám sa nepodarilo zväčšením počtu stromov dosiahnuť lepšie výsledky (optimálny počet bol ešte nižší) \n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "#vytvoríme klasifikátor\n",
        "clf=RandomForestClassifier(n_estimators=1000)\n",
        "\n",
        "#trénovanie modelu\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "y_pred=clf.predict(X_test)\n",
        "\n",
        "# pozrieme sa na presnosť modelu \n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "GJIuoLLr82Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7txHLhBdabHe"
      },
      "source": [
        "#***XGBoost***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCjIAUqOzsLb"
      },
      "outputs": [],
      "source": [
        "#túto bunku používať v prípade, že chceme pracovať s uloženým datasetom\n",
        "\n",
        "#načítanie datasetu \n",
        "X = pd.read_csv('top30_ExtraTreesClassifier.csv')\n",
        "#cieľová premenná \n",
        "y = X['isFraud']\n",
        "X = X.drop(columns=['isFraud'])\n",
        "\n",
        "#vytvorenie tréningovej a testovacej množiny, do tréningovej množiny sme vložili 67 % dát, zvyšok do testovacej, tento parameter tiež môžeme zmeniť. \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#túto bunku používať v prípade použitia datasetu s umelými podvodníkmi z časti Vytvorenie novej (rozšírenej) tréningovej množiny. \n",
        "#Tieto datasety by sa dali aj uložiť, kvôli šetreniu miesta sme ich neukladali stále zvlášť, názvy premenných sú nastavené tak, aby sa rovno dala použiť táto bunka \n",
        "\n",
        "y = y_train\n",
        "X = X_train\n",
        "\n",
        "#vytvorenie tréningovej a testovacej množiny, do tréningovej množiny sme vložili 67 % dát, zvyšok do testovacej, tento parameter tiež môžeme zmeniť. \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "eDay8SjbnUz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbUR8n6_aiPo",
        "outputId": "25292ca1-bf2e-4c7f-b757-356eef9111da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost version: 1.5.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/lubomir_antoni/.conda/envs/diplomove_vypocty/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\tvalidation_0-mae:0.49111\n",
            "[50]\tvalidation_0-mae:0.21764\n",
            "[100]\tvalidation_0-mae:0.11657\n",
            "[150]\tvalidation_0-mae:0.07838\n",
            "[200]\tvalidation_0-mae:0.06342\n",
            "[250]\tvalidation_0-mae:0.05722\n",
            "[300]\tvalidation_0-mae:0.05436\n",
            "[350]\tvalidation_0-mae:0.05276\n",
            "[400]\tvalidation_0-mae:0.05182\n",
            "[450]\tvalidation_0-mae:0.05114\n",
            "[500]\tvalidation_0-mae:0.05063\n",
            "[550]\tvalidation_0-mae:0.05014\n",
            "[600]\tvalidation_0-mae:0.04969\n",
            "[650]\tvalidation_0-mae:0.04937\n",
            "[700]\tvalidation_0-mae:0.04902\n",
            "[750]\tvalidation_0-mae:0.04871\n",
            "[800]\tvalidation_0-mae:0.04845\n",
            "[850]\tvalidation_0-mae:0.04821\n",
            "[900]\tvalidation_0-mae:0.04800\n",
            "[950]\tvalidation_0-mae:0.04773\n",
            "[1000]\tvalidation_0-mae:0.04750\n",
            "[1050]\tvalidation_0-mae:0.04728\n",
            "[1100]\tvalidation_0-mae:0.04709\n",
            "[1150]\tvalidation_0-mae:0.04689\n",
            "[1200]\tvalidation_0-mae:0.04670\n",
            "[1250]\tvalidation_0-mae:0.04653\n",
            "[1300]\tvalidation_0-mae:0.04637\n",
            "[1350]\tvalidation_0-mae:0.04620\n",
            "[1400]\tvalidation_0-mae:0.04604\n",
            "[1450]\tvalidation_0-mae:0.04586\n",
            "[1500]\tvalidation_0-mae:0.04573\n",
            "[1550]\tvalidation_0-mae:0.04558\n",
            "[1600]\tvalidation_0-mae:0.04544\n",
            "[1650]\tvalidation_0-mae:0.04535\n",
            "[1700]\tvalidation_0-mae:0.04521\n",
            "[1750]\tvalidation_0-mae:0.04509\n",
            "[1800]\tvalidation_0-mae:0.04496\n",
            "[1850]\tvalidation_0-mae:0.04486\n",
            "[1900]\tvalidation_0-mae:0.04474\n",
            "[1950]\tvalidation_0-mae:0.04465\n",
            "[1999]\tvalidation_0-mae:0.04460\n"
          ]
        }
      ],
      "source": [
        "#algoritmus XGBoost\n",
        "#môžeme zmeniť počet stromov, ich maximálnu hĺbku, učiaci parameter a iné. \n",
        "#ak chceme použiť GPU, tak treba odkomentovať riadok pod USE GPU a zakomentovať riadku pod USE CPU\n",
        "\n",
        "import xgboost as xgb\n",
        "print(\"XGBoost version:\", xgb.__version__)\n",
        "\n",
        "if BUILD95:\n",
        "    clf = xgb.XGBClassifier( \n",
        "        n_estimators=2000,\n",
        "        max_depth=12, \n",
        "        learning_rate=0.02, \n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.4, \n",
        "        eval_metric='mae',\n",
        "        # USE CPU\n",
        "        nthread=4,\n",
        "        tree_method='hist',\n",
        "        seed = 42 \n",
        "        # USE GPU\n",
        "        #tree_method='gpu_hist' \n",
        "    )\n",
        "    h = clf.fit(X_train, y_train, \n",
        "        eval_set=[(X_test,y_test)],\n",
        "        verbose=50, early_stopping_rounds=100)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "E4c2uK8jFruQ",
        "X8BbIXAQYr6G",
        "8pIAtjNxkZ85",
        "fDGAfUhl7GXg",
        "6MY43uNO4Nqs",
        "nd3KjD2zGHSe",
        "RWOAZ_qBZw_e",
        "P_n6qQF1bMV7",
        "oliiQue-R66S"
      ],
      "name": "Generatívne_superiace_siete_a_detekcia_anomalii_v_oblasti_bankovych_podvodov.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "diplomove_vypocty",
      "language": "python",
      "name": "diplomove_vypocty"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}